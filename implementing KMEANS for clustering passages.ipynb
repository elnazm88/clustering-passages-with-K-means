{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KKC3O3QiIi4"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**implementing the k-means clustering algorithm to cluster passages**\n",
    "\n",
    "The collections from the ANTIQUE  [https://arxiv.org/pdf/1905.08957.pdf] dataset have been used in this project.\n",
    "\n",
    "**Collection file**\n",
    "\n",
    "Each row of the file consists of the following information:\n",
    "\n",
    "*passage_id  passage_text*\n",
    "\n",
    "The id and text information is tab separated. The passage text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer. The terms in the passage text can be accessed by splitting the text based on space.\n",
    "\n",
    "**Centroid vector values**\n",
    "\n",
    "Each row is a tab separated entry where the first column is cluster id and the second column is the vector which is used to initialize the cluster centroid in the k-means Algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YM1igiBMcIB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Setting the input file\n",
    "passage = \"passages.tok.clean_kstem\"\n",
    "init_centroid_file = \"centroid.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7pEmMOeaoMa",
    "outputId": "c053cc32-5572-46f5-bfab-d1b4eefe0259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 3630\n",
      "Total Number of Passages: 500\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "In this function, load the initial centroid values for k clusters.  \n",
    "Return Variables: \n",
    "init_centroid_vec - array where each element corresponds to the centroid vector. \n",
    "'''\n",
    "\n",
    "# load initial centroid values\n",
    "def load_centroid_init(init_centroid_file):\n",
    "  init_centroid_vec = []\n",
    "  for line in open(init_centroid_file):\n",
    "   row = line.strip().split('\\t')\n",
    "   init_centroid_vec.append(np.fromstring(row[1][1:-1],sep=',').astype(float))\n",
    "  return init_centroid_vec \n",
    "\n",
    "\n",
    "''' \n",
    "In this function, iterate through the input passage and load the data  \n",
    "Return Variables: \n",
    "vocab - dict mapping word to an integer [0,len(vocab)]\n",
    "df - dict mapping word to document frquency\n",
    "num_passages - total number of passages in the input collection\n",
    "docs - dict mapping each passage id to unique integer between [0,len(num_passages)]\n",
    "'''\n",
    "# load vocabulary\n",
    "def load_vocab(passage):\n",
    "  df = {}\n",
    "  vocab = {}\n",
    "  count = 0\n",
    "  doc_count = 0\n",
    "  num_passages = 0\n",
    "  docs = {}\n",
    "  for line in open(passage):  \n",
    "   row = line.strip().split('\\t')\n",
    "   docs[row[0]] = doc_count\n",
    "   doc_count+=1\n",
    "   terms = row[1].split(' ')\n",
    "   num_passages+=1\n",
    "   for word in set(terms):\n",
    "    if word not in df:\n",
    "      df[word]=0\n",
    "    df[word]+=1\n",
    "   for word in terms:  \n",
    "    if word not in vocab: \n",
    "      vocab[word]=count\n",
    "      count+=1 \n",
    "\n",
    "  return vocab,df,num_passages,docs,doc_count     \n",
    "\n",
    "init_centroid_vec = load_centroid_init(init_centroid_file)\n",
    "vocab, df, num_passages, docs,doc_count = load_vocab(passage)\n",
    "\n",
    "print('Vocabulary Size : {0}'.format(len(vocab)))\n",
    "print('Total Number of Passages: {0}'.format(num_passages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq54VT8zcMjr"
   },
   "source": [
    "\n",
    "Create an input data matrix $P = m \\times n$  where $P_{i,j}=tf{\\text -}idf(i,j)$. The definition of the $tf{\\text -}idf(i,j)$ formulation is given below. \n",
    "\n",
    "$$ tf{\\text -}idf(i,j) = ln(1+count(i,j)) ln(1 + \\frac{|C|}{df(j)}) $$\n",
    "\n",
    "\n",
    "$tf{\\text -}idf(i,j)$: the  $tf{\\text -}idf$ value of the word corresponding to integer $j$ and passage corresponding to integer $i$ as defined in vocab and docs data structures, respectively. \n",
    "\n",
    "$count(i,j)$ = number of times word corresponding to integer $j$ occurs in passage corresponding to integer $i$.\n",
    "\n",
    "$df(j)$ - The document frequency of word corresponding to integer $j$ (i.e., the number of documents that contain the $j^{th}$ word.\n",
    "\n",
    "$|C|$ - Total number of passages in the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fIEB5l56-o-",
    "outputId": "b429b91d-0e0f-4e38-ab60-bcfc9383438d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input matrix : (500, 3630)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "In this function, create input matrix  \n",
    "Return Variables: \n",
    "pass_matrix - matrix of shape m by n where each row corresponds to a passage and each column to a word\n",
    "'''\n",
    "import math\n",
    "\n",
    "def create_input_matrix(passage,df,num_passages,vocab):\n",
    "  #create a dictionary from passage, which maps passage id to list of its words\n",
    "  passage_dict = {}\n",
    "  for line in open(passage):\n",
    "    row = line.strip().split('\\t')\n",
    "    passage_dict[row[0]]=row[1]\n",
    "  #create a tf-idf matrix\n",
    "  pass_matrix= np.zeros((num_passages, len(vocab)), dtype=\"float\")\n",
    "  C=num_passages\n",
    "  for id_original, id_integer in docs.items():\n",
    "    words= passage_dict[id_original].split(' ')\n",
    "    for word in words:\n",
    "      pass_matrix[id_integer][vocab[word]]= (math.log(1+words.count(word)))*(math.log(1+(C/df[word])))\n",
    "\n",
    "  return pass_matrix\n",
    "\n",
    "pass_matrix =  create_input_matrix(passage,df,num_passages,vocab)   \n",
    "\n",
    "print('Shape of the input matrix : {0}'.format(np.shape(pass_matrix)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK3G0NbfyHhp",
    "outputId": "ec85b57a-3731-4dc7-e36a-74fa6eca2505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.18964762, 2.60167109, 2.87728161, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.75058408, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [2.37929524, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 4.30902299,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        4.30902299]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V1g6gnbGrtR"
   },
   "source": [
    "\n",
    "implementing the k-means clustering algorithm with k=3(number of clusters). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6Sssau6MO-o",
    "outputId": "4278890a-729f-4aee-df08-1d656986069e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial centroids : 3\n",
      "----------------------------------\n",
      "number of elements in each cluster:\n",
      "1 128\n",
      "0 363\n",
      "2 9\n"
     ]
    }
   ],
   "source": [
    "# centroid is a matrix instantiated with initial centroid values\n",
    "''' \n",
    "In this function, centroid is instantiated with initial centroid values\n",
    "Here we assume for k=3, the cluster ids are {0,1,2}   \n",
    "Return Variables: \n",
    "centroid - matrix where rows correspond to clusters and columns correspond to words. \n",
    "'''\n",
    "def  init_centers(vocab, k, init_centroid_vec):\n",
    "  vocab_size = len(vocab)\n",
    "  centroid = np.zeros((k, vocab_size), dtype='float')\n",
    "  for i in range(len(init_centroid_vec)):\n",
    "     centroid[i] = init_centroid_vec[i]\n",
    "  return centroid   \n",
    "\n",
    "''' \n",
    "In this function, implement kmeans algorithm   \n",
    "Return Variables: \n",
    "final_cluster_assignment - array with cluster id values indexed by the passageid (integer mappings in docs)\n",
    "num_passage_cluster_final - dict with cluster-id and key and number of elements in the cluster as value\n",
    "'''\n",
    "def kmeans(num_passages, pass_matrix, centroid, k):\n",
    "  iteration=30\n",
    "  for iter in range(iteration):\n",
    "    #create a matrix to calculate the euclidean distance from each passage id and centroid\n",
    "    distance_matrix=np.zeros((num_passages,k))\n",
    "    for i in range(len(centroid)):\n",
    "      for j in range(len(pass_matrix)):\n",
    "        distance_matrix[j][i]=np.sqrt(np.sum((pass_matrix[j]-centroid[i])**2))\n",
    "    #assign cluster labels\n",
    "    cluster_labels = np.argmin(distance_matrix, axis = 1)\n",
    "    #update centroid at each iteration\n",
    "    centroid = np.asarray([pass_matrix[cluster_labels == i].mean(axis = 0) for i in range(k)])\n",
    "  cluster_labels_dict={}\n",
    "  for each_label in range(len(cluster_labels)):\n",
    "    if cluster_labels[each_label] not in cluster_labels_dict:\n",
    "      cluster_labels_dict[cluster_labels[each_label]]=[]\n",
    "      cluster_labels_dict[cluster_labels[each_label]].append(each_label)\n",
    "    else:\n",
    "      cluster_labels_dict[cluster_labels[each_label]].append(each_label)\n",
    "  \n",
    "  final_cluster_assignment_dict={}\n",
    "  for passageid, id_integer in docs.items():\n",
    "    for clusterid, id_integer1 in cluster_labels_dict.items():\n",
    "      for each_integer in id_integer1:\n",
    "        if each_integer == id_integer:\n",
    "          if clusterid not in final_cluster_assignment_dict:\n",
    "            final_cluster_assignment_dict[clusterid]=[]\n",
    "            final_cluster_assignment_dict[clusterid].append(passageid)\n",
    "          else:\n",
    "            final_cluster_assignment_dict[clusterid].append(passageid)\n",
    "  # create an array with cluster id values indexed by the passageid\n",
    "  final_cluster_assignment=np.array(final_cluster_assignment_dict.values())\n",
    "\n",
    "  num_passage_cluster_final={}\n",
    "  for each_cluster_id,passage_ids in final_cluster_assignment_dict.items():\n",
    "    num_passage_cluster_final[each_cluster_id]=len(passage_ids)\n",
    "    \n",
    "  return final_cluster_assignment,num_passage_cluster_final,final_cluster_assignment_dict\n",
    "\n",
    "centroid = init_centers(vocab, 3, init_centroid_vec)\n",
    "final_cluster_assignment, num_passage_cluster_final,final_cluster_assignment_dict = kmeans(num_passages, pass_matrix, centroid, 3)\n",
    "\n",
    "print('Number of initial centroids : {0}'.format(len(centroid)) )\n",
    "print(\"----------------------------------\")\n",
    "print(\"number of elements in each cluster:\")\n",
    "\n",
    "for cid,cnum in num_passage_cluster_final.items():\n",
    "   print(cid, cnum)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pyCSMB1MOaA"
   },
   "source": [
    "**Evaluation**\n",
    "\n",
    "1) IntraCluster Similarity Metric - Average Diameter \n",
    "Distance of each cluster $S$\n",
    "\n",
    "$$Sim(S) = \\frac{1}{|S| (|S|-1)} \\sum_{x,y \\in S,x \\neq y} dist(x,y)$$\n",
    "\n",
    "$|S|$ - number of passages assigned to cluster $S$\n",
    "\n",
    "$dist(x,y)$ - the squared euclidean distance between passages $x$ and $y$.\n",
    "\n",
    "\n",
    "2) Intercluster Similarity Metric - \n",
    "Average Linkage Distance between a pair of clusters $S,T$\n",
    "\n",
    "$$Sim(S,T) = \\frac{1}{|S| |T|} \\sum_{x \\in S,y \\in T} dist(x,y)$$\n",
    "\n",
    "$|S|$ - number of passages assigned to cluster $S$\n",
    "\n",
    "$|T|$ - number of passages assigned to cluster $T$\n",
    "\n",
    "$dist(x,y)$ - the squared euclidean distance between passages $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQ8S-4ybMXgu",
    "outputId": "db25142d-dc56-4c56-81ee-5a0240ed2a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intracluster scores for very cluster:\n",
      "1 1140.0375066528834\n",
      "0 319.1771238766518\n",
      "2 3867.7247873603037\n",
      "-----------------------------------------------------\n",
      "cluster 0 is the largest, cluster 1 is the second largest and cluster 2 is the smallest\n",
      "\n",
      "intercluster scores for every pair of cluster:\n",
      "(1, 0) 744.3454376966346\n",
      "(1, 2) 2598.1074296167694\n",
      "(0, 2) 2268.718168712698\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "In this function, implement intracluster similarity metric Average Diameter Distance   \n",
    "Return Variables: \n",
    "sim_score - dict with metric score corresponding to each of the three clusters\n",
    "'''\n",
    "def average_diameter_dist(num_passage_cluster_final,final_cluster_assignment,pass_matrix,final_cluster_assignment_dict):\n",
    "  sim_score={}\n",
    "  for clusterid , passages in final_cluster_assignment_dict.items():\n",
    "    sigma=0\n",
    "    for each_passage_i in passages:\n",
    "      for each_passage_j in passages:\n",
    "        #not for the same passageid\n",
    "        if each_passage_i != each_passage_j:\n",
    "          #claculate distance of each passageid in each cluster\n",
    "          sigma+=(np.sum(np.square((pass_matrix[docs[each_passage_i]]\n",
    "                                    -pass_matrix[docs[each_passage_j]]))))\n",
    "    result=sigma/(num_passage_cluster_final[clusterid]\n",
    "                  *(num_passage_cluster_final[clusterid]-1))\n",
    "    sim_score[clusterid]=result\n",
    "   \n",
    "  return sim_score\n",
    "\n",
    "''' \n",
    "In this function, implement intercluster similarity metric Average Linkage Distance   \n",
    "Return Variables: \n",
    "sim_score - dict with metric score corresponding to each pair of clusters\n",
    "'''\n",
    "def average_linkage_dist(num_passage_cluster_final,final_cluster_assignment,pass_matrix,final_cluster_assignment_dict):\n",
    "  sim_score={}\n",
    "  for clusterid , passages in final_cluster_assignment_dict.items():\n",
    "    for clusterid_second , passages_second in final_cluster_assignment_dict.items():\n",
    "      sigma=0\n",
    "      if clusterid != clusterid_second:\n",
    "        for each_passage_i in passages:\n",
    "          for each_passage_j in passages_second:\n",
    "            if each_passage_i != each_passage_j:\n",
    "              sigma+=(np.sum(np.square((pass_matrix[docs[each_passage_i]]\n",
    "                                        -pass_matrix[docs[each_passage_j]]))))\n",
    "      #similarity for pairs of cluster\n",
    "      if clusterid != clusterid_second and (clusterid_second, clusterid) not in sim_score:\n",
    "        result = sigma/(num_passage_cluster_final[clusterid]\n",
    "                        *(num_passage_cluster_final[clusterid_second]))\n",
    "\n",
    "        sim_score[(clusterid, clusterid_second)]=result\n",
    "   \n",
    "  return sim_score\n",
    "\n",
    "intra_score  = average_diameter_dist(num_passage_cluster_final,final_cluster_assignment,pass_matrix,final_cluster_assignment_dict) \n",
    "inter_score =  average_linkage_dist(num_passage_cluster_final,final_cluster_assignment,pass_matrix,final_cluster_assignment_dict)  \n",
    "\n",
    "print(\"intracluster scores for very cluster:\")\n",
    "for cid,score in intra_score.items():\n",
    "   print(cid, score) \n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"cluster 0 is the largest, cluster 1 is the second largest and cluster 2 is the smallest\\n\")\n",
    "print(\"intercluster scores for every pair of cluster:\")\n",
    "\n",
    "for cid_pair,score in inter_score.items():\n",
    "   print(cid_pair, score)    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
